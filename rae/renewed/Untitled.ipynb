{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29447776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metric\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import sklearn.preprocessing as pre\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import wrangle as w\n",
    "import explore as e\n",
    "import model as m\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be81494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explore_data():\n",
    "    ''' \n",
    "    This function reads in a csv held in the same repository folder\n",
    "    '''\n",
    "    df = pd.read_csv('train_data.csv').drop('index', axis='columns')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601776d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df, features=[]):\n",
    "    '''\n",
    "    This function pulls in the defined features as the only features \n",
    "    to be represented as columns in the resulting dataframe\n",
    "    '''\n",
    "    df['startdate'] = pd.to_datetime(df['startdate'])\n",
    "    if len(features) == 0:\n",
    "        return df\n",
    "    else:\n",
    "        return df[features]\n",
    "\n",
    "def create_region_bins(df):\n",
    "    '''\n",
    "    This function creates a new column that holds\n",
    "    three categorical variables dry, temperate, and continental\n",
    "    that represents the bins we put the 15 original regions into\n",
    "    based on the first letter of their Koppen-Geiger code\n",
    "    '''\n",
    "    df['region_bins'] = df.region.replace({'BWh' :'Dry', 'BWk' :'Dry', 'BSh':'Dry', 'BSk':'Dry',\n",
    "                                        'Csa':'Temperate', 'Csb':'Temperate', 'Cfa':'Temperate', 'Cfb':'Temperate',\n",
    "                                        'Dsb':'Continental', 'Dsc':'Continental', 'Dwa':'Continental', 'Dwb':'Continental', 'Dfa':'Continental', 'Dfb':'Continental', 'Dfc':'Continental'})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_elevation_bins(df):\n",
    "    '''\n",
    "    Function creates four bins of elevation based\n",
    "    on mathematical quantiles.\n",
    "    '''\n",
    "    names = ['bottom_low', 'top_low', 'mid', 'high']\n",
    "    df['elevation_range'] = pd.qcut(df['elevation'], 4, labels=names)\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "def create_season_bins(df):\n",
    "    '''\n",
    "    Creates a column for month and then bases that to create bins for seasons\n",
    "    '''\n",
    "    #create month column\n",
    "    df['month']=df['startdate'].dt.month\n",
    "    \n",
    "    #define season groups\n",
    "    season_groups = {\n",
    "        \"Autumn\": [9,10,11],\n",
    "        \"Winter\": [12,1,2],\n",
    "        \"Spring\": [3,4,5],\n",
    "        \"Summer\": [6,7,8],\n",
    "    }\n",
    "    #add season onto the df\n",
    "    df[\"season\"] = (\n",
    "        df[\"month\"]\n",
    "        .apply(lambda x: [k for k in season_groups.keys() if x in season_groups[k]])\n",
    "        .str[0]\n",
    "        .fillna(\"Other\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def rename_data(df):\n",
    "    '''\n",
    "    This function takes in the dataframe and returns all columns\n",
    "    with only the listed columns names changed to be more readable.\n",
    "    '''\n",
    "    df=df.rename(columns={'climateregions__climateregion': 'region', \n",
    "                                      'elevation__elevation': 'elevation',\n",
    "                                      'contest-pevpr-sfc-gauss-14d__pevpr':'potential_evap',\n",
    "                                      'contest-precip-14d__precip':'precip',\n",
    "                                      'contest-pres-sfc-gauss-14d__pres':'barometric_pressure',\n",
    "                                      'contest-prwtr-eatm-14d__prwtr':'all_atmos_precip',\n",
    "                                      'contest-rhum-sig995-14d__rhum':'relative_humidity',\n",
    "                                      'contest-slp-14d__slp':'sea_level_press',\n",
    "                                      'contest-tmp2m-14d__tmp2m':'mean_temp',\n",
    "                                      'contest-wind-h10-14d__wind-hgt-10':'height_10_mb',\n",
    "                                      'contest-wind-h100-14d__wind-hgt-100':'height_100_mb',\n",
    "                                      'contest-wind-h500-14d__wind-hgt-500':'height_500_mb',\n",
    "                                      'contest-wind-h850-14d__wind-hgt-850':'height_850_mb',\n",
    "                                      'contest-wind-uwnd-250-14d__wind-uwnd-250':'zonal_wind_250mb',\n",
    "                                      'contest-wind-uwnd-925-14d__wind-uwnd-925':'zonal_wind_925mb',\n",
    "                                      'contest-wind-vwnd-250-14d__wind-vwnd-250':'long_wind_250mb',\n",
    "                                      'contest-wind-vwnd-925-14d__wind-vwnd-925':'long_wind_925mb'\n",
    "                                     })\n",
    "    return df\n",
    "\n",
    "def deal_with_nulls(df):\n",
    "    '''\n",
    "    Calculates nulls based on other features in the df\n",
    "    and drops one that can't be computed\n",
    "    '''\n",
    "    #calculate the columns with nulls\n",
    "    count_na_df = df.isna().sum()\n",
    "    null_cols = list(count_na_df[count_na_df > 0].index)\n",
    "\n",
    "    #means of each nmme measurement\n",
    "    g_means =  ['nmme0-tmp2m-34w__nmme0mean', \n",
    "    'nmme-tmp2m-56w__nmmemean', \n",
    "    'nmme-prate-34w__nmmemean', \n",
    "    'nmme0-prate-56w__nmme0mean', \n",
    "    'nmme0-prate-34w__nmme0mean', \n",
    "    'nmme-prate-56w__nmmemean', \n",
    "    'nmme-tmp2m-34w__nmmemean']\n",
    "\n",
    "    #measurements we have already\n",
    "    g_1 = ['nmme0-tmp2m-34w__cancm30',\n",
    "    'nmme0-tmp2m-34w__cancm40',\n",
    "    'nmme0-tmp2m-34w__ccsm40',\n",
    "    'nmme0-tmp2m-34w__cfsv20',\n",
    "    'nmme0-tmp2m-34w__gfdlflora0',\n",
    "    'nmme0-tmp2m-34w__gfdlflorb0',\n",
    "    'nmme0-tmp2m-34w__gfdl0',\n",
    "    'nmme0-tmp2m-34w__nasa0']\n",
    "\n",
    "    g_2 = ['nmme-tmp2m-56w__cancm3',\n",
    "    'nmme-tmp2m-56w__cancm4',\n",
    "    'nmme-tmp2m-56w__ccsm4',\n",
    "    'nmme-tmp2m-56w__cfsv2',\n",
    "    'nmme-tmp2m-56w__gfdl',\n",
    "    'nmme-tmp2m-56w__gfdlflora',\n",
    "    'nmme-tmp2m-56w__gfdlflorb',\n",
    "    'nmme-tmp2m-56w__nasa']\n",
    "\n",
    "    g_3 = ['nmme-prate-34w__cancm3',\n",
    "    'nmme-prate-34w__cancm4',\n",
    "    'nmme-prate-34w__ccsm4',\n",
    "    'nmme-prate-34w__cfsv2',\n",
    "    'nmme-prate-34w__gfdl',\n",
    "    'nmme-prate-34w__gfdlflora',\n",
    "    'nmme-prate-34w__gfdlflorb',\n",
    "    'nmme-prate-34w__nasa']\n",
    "\n",
    "    g_4 = [ 'nmme0-prate-56w__cancm30',\n",
    "    'nmme0-prate-56w__cancm40',\n",
    "    'nmme0-prate-56w__ccsm40',\n",
    "    'nmme0-prate-56w__cfsv20',\n",
    "    'nmme0-prate-56w__gfdlflora0',\n",
    "    'nmme0-prate-56w__gfdlflorb0',\n",
    "    'nmme0-prate-56w__gfdl0',\n",
    "    'nmme0-prate-56w__nasa0']\n",
    "\n",
    "    g_5 = ['nmme0-prate-34w__cancm30',\n",
    "    'nmme0-prate-34w__cancm40',\n",
    "    'nmme0-prate-34w__ccsm40',\n",
    "    'nmme0-prate-34w__cfsv20',\n",
    "    'nmme0-prate-34w__gfdlflora0',\n",
    "    'nmme0-prate-34w__gfdlflorb0',\n",
    "    'nmme0-prate-34w__gfdl0',\n",
    "    'nmme0-prate-34w__nasa0']\n",
    "\n",
    "    g_6 = ['nmme-prate-56w__cancm3',\n",
    "    'nmme-prate-56w__cancm4',\n",
    "    'nmme-prate-56w__ccsm4',\n",
    "    'nmme-prate-56w__cfsv2',\n",
    "    'nmme-prate-56w__gfdl',\n",
    "    'nmme-prate-56w__gfdlflora',\n",
    "    'nmme-prate-56w__gfdlflorb',\n",
    "    'nmme-prate-56w__nasa']\n",
    "\n",
    "    g_7 = ['nmme-tmp2m-34w__cancm3',\n",
    "    'nmme-tmp2m-34w__cancm4',\n",
    "    'nmme-tmp2m-34w__ccsm4',\n",
    "    'nmme-tmp2m-34w__cfsv2',\n",
    "    'nmme-tmp2m-34w__gfdl',\n",
    "    'nmme-tmp2m-34w__gfdlflora',\n",
    "    'nmme-tmp2m-34w__gfdlflorb',\n",
    "    'nmme-tmp2m-34w__nasa']\n",
    "\n",
    "    #make a list of lists\n",
    "    gs = [g_1, g_2, g_3, g_4, g_5, g_6, g_7]\n",
    "\n",
    "    #go through and compute all features where there were nulls\n",
    "    zip_cols = zip(null_cols, gs, g_means)\n",
    "    for c, g, s in zip_cols:\n",
    "        df[c] = (df[s]*9) - df[g].sum(1)\n",
    "\n",
    "    #drop column we can't compute\n",
    "    df = df.drop(columns='ccsm30')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "################################# SUM OF PREPARATION \n",
    "def get_contest_data(df):\n",
    "    '''\n",
    "    This function takes in a dataframe and runs it \n",
    "    through the processes of superior preparation functions.\n",
    "    '''\n",
    "    df = prep_data(df, features=features)\n",
    "    df = rename_data(df)\n",
    "    df = create_elevation_bins(df)\n",
    "    df = create_region_bins(df)\n",
    "\n",
    "    #df = df.drop(columns=['elevation','region']\n",
    "    return df\n",
    "    \n",
    "################################################################## SPLITTING DATA\n",
    "def split_data(df, test_size=0.15):\n",
    "    '''\n",
    "    Takes in a data frame and the train size\n",
    "    It returns train, validate , and test data frames\n",
    "    with validate being 0.05 bigger than test and train has the rest of the data.\n",
    "    '''\n",
    "    train, test = train_test_split(df, test_size = test_size , random_state=27)\n",
    "    train, validate = train_test_split(train, test_size = (test_size + 0.05)/(1-test_size), random_state=27)\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4178f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquiring data\n",
    "df = w.get_explore_data()\n",
    "\n",
    "# prepping data\n",
    "df = w.get_contest_data(df)\n",
    "\n",
    "# splitting data into train, validate, and test\n",
    "train, validate, test = w.split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_distribution(df):\n",
    "    #set font size\n",
    "    sns.set(font_scale=1.5)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 8))\n",
    "    sns.histplot(data=df, x='mean_temp')\n",
    "    plt.xlabel('Mean Temp for Next 14 days')\n",
    "    plt.title('Distribution of our Target Variable');\n",
    "    \n",
    "def region_viz(df):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    fig.suptitle('Is there a relationship between climate region and the mean temperature?')\n",
    "    \n",
    "    sns.countplot(x='region', data=df, ax=ax[0])\n",
    "    ax[0].set_title('Distribution of Regions')\n",
    "\n",
    "    sns.barplot(x='region', y='mean_temp', data=df, ax=ax[1])\n",
    "    rate = df['mean_temp'].mean()\n",
    "    ax[1].set_title('Mean temp across regions')\n",
    "    ax[1].axhline(rate,  label = f'Average Temp Across All Regions {rate:.2f}', linestyle='dotted', color='black')\n",
    "    ax[1].legend()\n",
    "    plt.show()\n",
    "\n",
    "def region_stats_test(df):\n",
    "    dry = df[df.region_bins == 'Dry']\n",
    "    temp = df[df.region_bins == 'Temperate']\n",
    "    cont = df[df.region_bins == 'Continental']\n",
    "    \n",
    "    corr, p = stats.kruskal(dry.mean_temp, temp.mean_temp, cont.mean_temp)\n",
    "    \n",
    "    print(f'p-value: {p}')\n",
    "    \n",
    "\n",
    "def elevation_bin_viz(df):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    fig.suptitle('Is there a relationship between elevation_range and the mean temperature?')\n",
    "    \n",
    "    sns.countplot(x='elevation_range', data=df, ax=ax[0])\n",
    "    ax[0].set_title('Distribution of Elevation')\n",
    "\n",
    "    sns.barplot(x='elevation_range', y='mean_temp', data=df, ax=ax[1])\n",
    "    rate = df['mean_temp'].mean()\n",
    "    ax[1].set_title('Mean temp across elevation bins')\n",
    "    ax[1].axhline(rate,  label = f'Average Temp Across All Elevations {rate:.2f}', linestyle='dotted', color='black')\n",
    "    ax[1].legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def elevation_bin_kruskal_test(df):\n",
    "    bl = df[df.elevation_range == 'bottom_low']\n",
    "    tl = df[df.elevation_range == 'top_low']\n",
    "    mid = df[df.elevation_range == 'mid']\n",
    "    h = df[df.elevation_range == 'high']\n",
    "\n",
    "    corr, p = stats.kruskal(bl.mean_temp, tl.mean_temp, mid.mean_temp, h.mean_temp)\n",
    "    \n",
    "    print(f'p-value: {p}')\n",
    "\n",
    "\n",
    "def elevation_bin_dist_viz(df):\n",
    "    bl = df[df.elevation_range == 'bottom_low']\n",
    "    tl = df[df.elevation_range == 'top_low']\n",
    "    mid = df[df.elevation_range == 'mid']\n",
    "    h = df[df.elevation_range == 'high']\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 4, figsize=(20, 8))\n",
    "\n",
    "    fig.suptitle('Distribution of Observations by Elevation Bin')\n",
    "\n",
    "    sns.histplot(x='mean_temp', data= bl, ax=ax[0])\n",
    "    ax[0].set_title('Bottom Low')\n",
    "    ax[0].set_ylim(0,2000)\n",
    "    ax[0].set_xlim(-20,40)\n",
    "    ax[0].set_ylabel('')\n",
    "\n",
    "    sns.histplot(x='mean_temp', data= tl, ax=ax[1])\n",
    "    ax[1].set_title('Top Low')\n",
    "    ax[1].set_ylim(0,2000)\n",
    "    ax[1].set_xlim(-20,40)\n",
    "    ax[1].set_ylabel('')\n",
    "\n",
    "    sns.histplot(x='mean_temp', data= mid, ax=ax[2])\n",
    "    ax[2].set_title('Mid')\n",
    "    ax[2].set_ylim(0,2000)\n",
    "    ax[2].set_xlim(-20,40)\n",
    "    ax[2].set_ylabel('')\n",
    "\n",
    "    sns.histplot(x='mean_temp', data= h, ax=ax[3])\n",
    "    ax[3].set_title('High')\n",
    "    ax[3].set_ylim(0,2000)\n",
    "    ax[3].set_xlim(-20,40)\n",
    "    ax[3].set_ylabel('')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def precipitation_viz(df):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    sns.histplot(data=df, x ='precip', ax=ax[0])\n",
    "    ax[0].set_title('Distribution of Precipitation')\n",
    "\n",
    "    sns.regplot(x='mean_temp', y='precip', data=df, line_kws={'color': 'red'}, ax=ax[1])\n",
    "    ax[1].set_title('Is there a correlation between mean temp and precipitation?')\n",
    "    rate = df['precip'].mean()\n",
    "    ax[1].axhline(rate,  label = f'Overall Mean Precipitation: {rate:.2f}', linestyle='dotted', color='black')\n",
    "    ax[1].legend()\n",
    "    plt.show()\n",
    "    \n",
    "def precip_spearmanr_test(df):\n",
    "    corr, p = stats.spearmanr(df['precip'], df['mean_temp'])\n",
    "    print(f'p-value: {p}')\n",
    "\n",
    "    \n",
    "def potential_evap_viz(df):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    sns.histplot(data=df, x ='potential_evap', ax=ax[0])\n",
    "    ax[0].set_title('Distribution of Potential Evaporation')\n",
    "\n",
    "    sns.regplot(x='mean_temp', y='potential_evap', data=df, line_kws={'color': 'red'}, ax=ax[1])\n",
    "    ax[1].set_title('Is there a correlation between mean temp and potential evaporation?')\n",
    "    rate = df['potential_evap'].mean()\n",
    "    ax[1].axhline(rate,  label = f'Overall Mean Potential Evaporation: {rate:.2f}', linestyle='dotted', color='black')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_ylim(-50,1200)\n",
    "    plt.show()\n",
    "    \n",
    "def potential_evap_spearmanr_test(df):\n",
    "    corr, p = stats.spearmanr(df['potential_evap'], df['mean_temp'])\n",
    "    print(f'p-value: {p}')\n",
    "    \n",
    "def geopotential_viz(df):\n",
    "    rows = df.sample(frac =.01)\n",
    "    \n",
    "    fig, ax = plt.subplots(2, 2, figsize=(20, 20))\n",
    "\n",
    "    fig.suptitle('Is there a correlation between mean temp and geopotential pressure at different heights?')\n",
    "\n",
    "    sns.scatterplot(x='height_10_mb', y='mean_temp', hue='region_bins', data= rows, ax=ax[0][0])\n",
    "    ax[0][0].set_title('At 10 Milibars')\n",
    "    ax[0][0].legend()\n",
    "\n",
    "    sns.scatterplot(x='height_100_mb', y='mean_temp', hue='region_bins', data= rows, ax=ax[0][1])\n",
    "    ax[0][1].set_title('At 100 Milibars')\n",
    "    ax[0][1].legend()\n",
    "\n",
    "    sns.scatterplot(x='height_500_mb', y='mean_temp', hue='region_bins', data= rows, ax=ax[1][0])\n",
    "    ax[1][0].set_title('At 500 Milibars')\n",
    "    ax[1][0].legend()\n",
    "\n",
    "    sns.scatterplot(x='height_850_mb', y='mean_temp', hue='region_bins', data= rows, ax=ax[1][1])\n",
    "    ax[1][1].set_title('At 850 Milibars')\n",
    "    ax[1][1].legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3426b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(train, validate, test):\n",
    "    '''\n",
    "    Takes in train, validate, test and a list of features to scale\n",
    "    and scales those features.\n",
    "    Returns df with new columns with scaled data\n",
    "    '''\n",
    "    scale_features= list(train.select_dtypes(include=np.number).columns)\n",
    "    \n",
    "    train_scaled = train.copy()\n",
    "    validate_scaled = validate.copy()\n",
    "    test_scaled = test.copy()\n",
    "    \n",
    "    minmax = pre.MinMaxScaler()\n",
    "    minmax.fit(train[scale_features])\n",
    "    \n",
    "    train_scaled[scale_features] = pd.DataFrame(minmax.transform(train[scale_features]),\n",
    "                                                  columns=train[scale_features].columns.values).set_index([train.index.values])\n",
    "                                                  \n",
    "    validate_scaled[scale_features] = pd.DataFrame(minmax.transform(validate[scale_features]),\n",
    "                                               columns=validate[scale_features].columns.values).set_index([validate.index.values])\n",
    "    \n",
    "    test_scaled[scale_features] = pd.DataFrame(minmax.transform(test[scale_features]),\n",
    "                                                 columns=test[scale_features].columns.values).set_index([test.index.values])\n",
    "    \n",
    "    return train_scaled, validate_scaled, test_scaled\n",
    "\n",
    "def prep_for_model(train, validate, test, target, drivers):\n",
    "    '''\n",
    "    Takes in train, validate, and test data frames\n",
    "    then splits  for X (all variables but target variable) \n",
    "    and y (only target variable) for each data frame\n",
    "    '''\n",
    "    #scale data\n",
    "    train_scaled, validate_scaled, test_scaled = scale_data(train, validate, test)\n",
    "    \n",
    "    X_train = train_scaled[drivers]\n",
    "    \n",
    "    #make list of cat variables to make dummies for\n",
    "    cat_vars = list(X_train.select_dtypes(exclude=np.number).columns)\n",
    "\n",
    "    dummy_df_train = pd.get_dummies(X_train[cat_vars], dummy_na=False, drop_first=[True, True])\n",
    "    X_train = pd.concat([X_train, dummy_df_train], axis=1).drop(columns=cat_vars)\n",
    "    y_train = train[target]\n",
    "\n",
    "    X_validate = validate_scaled[drivers]\n",
    "    dummy_df_validate = pd.get_dummies(X_validate[cat_vars], dummy_na=False, drop_first=[True, True])\n",
    "    X_validate = pd.concat([X_validate, dummy_df_validate], axis=1).drop(columns=cat_vars)\n",
    "    y_validate = validate[target]\n",
    "\n",
    "    X_test = test_scaled[drivers]\n",
    "    dummy_df_test = pd.get_dummies(X_test[cat_vars], dummy_na=False, drop_first=[True, True])\n",
    "    X_test = pd.concat([X_test, dummy_df_test], axis=1).drop(columns=cat_vars)\n",
    "    y_test = test[target]\n",
    "\n",
    "    return X_train, y_train, X_validate, y_validate, X_test, y_test\n",
    "\n",
    "def regression_models(X_train, y_train, X_validate, y_validate):\n",
    "    '''\n",
    "    Takes in X_train, y_train, X_validate, y_validate and runs \n",
    "    different models and produces df with RMSE and r^2 scores\n",
    "    for each model on train and validate.\n",
    "    '''\n",
    "    \n",
    "    train_predictions = pd.DataFrame(y_train)\n",
    "    validate_predictions = pd.DataFrame(y_validate)\n",
    "\n",
    "    # create the metric_df as a blank dataframe\n",
    "    metric_df = pd.DataFrame() \n",
    "\n",
    "    #OLS Model\n",
    "    lm = LinearRegression(normalize=True)\n",
    "    lm.fit(X_train, y_train)\n",
    "    train_predictions['lm'] = lm.predict(X_train)\n",
    "    # predict validate\n",
    "    validate_predictions['lm'] = lm.predict(X_validate)\n",
    "    metric_df = make_metric_df(y_train, train_predictions.lm, y_validate, validate_predictions.lm, metric_df, model_name = 'OLS Regressor')\n",
    "\n",
    "    #Lasso Lars\n",
    "    # create the model object\n",
    "    lars = LassoLars(alpha=1)\n",
    "    lars.fit(X_train, y_train)\n",
    "    # predict train\n",
    "    train_predictions['lars'] = lars.predict(X_train)\n",
    "    # predict validate\n",
    "    validate_predictions['lars'] = lars.predict(X_validate)\n",
    "    metric_df = make_metric_df(y_train, train_predictions.lars, y_validate, validate_predictions.lars, metric_df, model_name = 'Lasso_alpha_1')\n",
    "\n",
    "    # make the polynomial features to get a new set of features\n",
    "    pf = PolynomialFeatures(degree=2)\n",
    "    # fit and transform X_train_scaled\n",
    "    X_train_degree2 = pf.fit_transform(X_train)\n",
    "    # transform X_validate_scaled & X_test_scaled\n",
    "    X_validate_degree2 = pf.transform(X_validate)\n",
    "    # create the model object\n",
    "    lm2 = LinearRegression(normalize=True)\n",
    "    lm2.fit(X_train_degree2, y_train)\n",
    "    # predict train\n",
    "    train_predictions['poly_2'] = lm2.predict(X_train_degree2)\n",
    "    # predict validate\n",
    "    validate_predictions['poly_2'] = lm2.predict(X_validate_degree2)\n",
    "    metric_df = make_metric_df(y_train, train_predictions.poly_2, y_validate, validate_predictions.poly_2, metric_df, model_name = 'Quadratic')\n",
    "\n",
    "    return metric_df\n",
    "\n",
    "def make_metric_df(y_train, y_train_pred, y_validate, y_validate_pred,  metric_df,model_name ):\n",
    "    '''\n",
    "    Takes in y_train, y_train_pred, y_validate, y_validate_pred, and a df\n",
    "    returns a df of RMSE and r^2 score for the model on train and validate\n",
    "    '''\n",
    "    if metric_df.size ==0:\n",
    "        metric_df = pd.DataFrame(data=[\n",
    "            {\n",
    "                'model': model_name, \n",
    "                f'RMSE_train': metric.mean_squared_error(\n",
    "                    y_train,\n",
    "                    y_train_pred) ** .5,\n",
    "                f'RMSE_validate': metric.mean_squared_error(\n",
    "                    y_validate,\n",
    "                    y_validate_pred) ** .5\n",
    "            }])\n",
    "        return metric_df\n",
    "    else:\n",
    "        return metric_df.append(\n",
    "            {\n",
    "                'model': model_name, \n",
    "                f'RMSE_train': metric.mean_squared_error(\n",
    "                    y_train,\n",
    "                    y_train_pred) ** .5,\n",
    "                f'RMSE_validate': metric.mean_squared_error(\n",
    "                    y_validate,\n",
    "                    y_validate_pred) ** .5\n",
    "            }, ignore_index=True)\n",
    "\n",
    "def baseline_models(y_train, y_validate):\n",
    "    '''\n",
    "    Takes in y_train and y_validate and returns a df of \n",
    "    baseline_mean and baseline_median and how they perform\n",
    "    '''\n",
    "    train_predictions = pd.DataFrame(y_train)\n",
    "    validate_predictions = pd.DataFrame(y_validate)\n",
    "    \n",
    "    y_pred_mean = y_train.mean()\n",
    "    train_predictions['y_pred_mean'] = y_pred_mean\n",
    "    validate_predictions['y_pred_mean'] = y_pred_mean\n",
    "\n",
    "\n",
    "    # create the metric_df as a blank dataframe\n",
    "    metric_df = pd.DataFrame(data=[\n",
    "    {\n",
    "        'model': 'mean_baseline', \n",
    "        'RMSE_train': metric.mean_squared_error(\n",
    "            y_train,\n",
    "            train_predictions['y_pred_mean']) ** .5,\n",
    "        'RMSE_validate': metric.mean_squared_error(\n",
    "            y_validate,\n",
    "            validate_predictions['y_pred_mean']) ** .5,\n",
    "    }])\n",
    "\n",
    "    return metric_df\n",
    "\n",
    "def best_model(X_train, y_train, X_validate, y_validate, X_test, y_test):\n",
    "    '''\n",
    "    Takes in X_train, y_train, X_validate, y_validate, X_test, y_test\n",
    "    and returns a df with the RMSE and r^2 score on train, validate and test\n",
    "    '''    \n",
    "    # make the polynomial features to get a new set of features\n",
    "    pf = PolynomialFeatures(degree=2)\n",
    "    # fit and transform X_train_scaled\n",
    "    X_train_degree2 = pf.fit_transform(X_train)\n",
    "    # transform X_validate_scaled & X_test_scaled\n",
    "    X_validate_degree2 = pf.transform(X_validate)\n",
    "    X_test_degree2 = pf.transform(X_test)\n",
    "    # create the model object\n",
    "    lm2 = LinearRegression(normalize=True)\n",
    "    lm2.fit(X_train_degree2, y_train)\n",
    "    \n",
    "    metric_df = pd.DataFrame(data=[\n",
    "            {\n",
    "                'model': 'Quadratic', \n",
    "                f'RMSE_train': metric.mean_squared_error(\n",
    "                    y_train,\n",
    "                    lm2.predict(X_train_degree2)) ** .5,\n",
    "                f'RMSE_validate': metric.mean_squared_error(\n",
    "                    y_validate,\n",
    "                    lm2.predict(X_validate_degree2)) ** .5,\n",
    "                f'RMSE_test': metric.mean_squared_error(\n",
    "                    y_test,\n",
    "                    lm2.predict(X_test_degree2)) ** .5,\n",
    "            }])\n",
    "    \n",
    "    return metric_df\n",
    "\n",
    "\n",
    "def scale_kaggle_data(train, test, target):\n",
    "    '''\n",
    "    Takes in train/test dfs from kaggle and target.\n",
    "    Returns dfs with continuous variables scaled\n",
    "    '''\n",
    "    scale_features= list(train.select_dtypes(include=np.number).columns)\n",
    "    scale_features.remove(target)\n",
    "    \n",
    "    train_scaled = train.copy()\n",
    "    test_scaled = test.copy()\n",
    "    \n",
    "    minmax = pre.MinMaxScaler()\n",
    "    minmax.fit(train[scale_features])\n",
    "    \n",
    "    train_scaled[scale_features] = pd.DataFrame(minmax.transform(train[scale_features]),\n",
    "                                                  columns=train[scale_features].columns.values).set_index([train.index.values])\n",
    "    \n",
    "    test_scaled[scale_features] = pd.DataFrame(minmax.transform(test[scale_features]),\n",
    "                                                 columns=test[scale_features].columns.values).set_index([test.index.values])\n",
    "    \n",
    "    return train_scaled, test_scaled\n",
    "\n",
    "def prep_for_kaggle(train, test, target, drivers):\n",
    "    '''\n",
    "    Takes in train/test dfs from kaggle, target variable, and list of drivers\n",
    "    then splits  for X (all variables but target variable) \n",
    "    and y (only target variable) for each data frame\n",
    "    '''\n",
    "    #scale data\n",
    "    train_scaled, test_scaled = scale_kaggle_data(train, test, target)\n",
    "    \n",
    "    X_train = train_scaled[drivers]\n",
    "    \n",
    "    #make list of cat variables to make dummies for\n",
    "    cat_vars = list(X_train.select_dtypes(exclude=np.number).columns)\n",
    "\n",
    "    dummy_df_train = pd.get_dummies(X_train[cat_vars], dummy_na=False, drop_first=[True, True])\n",
    "    X_train = pd.concat([X_train, dummy_df_train], axis=1).drop(columns=cat_vars)\n",
    "    y_train = train[target]\n",
    "\n",
    "    X_test = test_scaled[drivers]\n",
    "    dummy_df_test = pd.get_dummies(X_test[cat_vars], dummy_na=False, drop_first=[True, True])\n",
    "    X_test = pd.concat([X_test, dummy_df_test], axis=1).drop(columns=cat_vars)\n",
    "    #y_test = test[target]\n",
    "\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed68f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the number of folds for the cross-validation\n",
    "n_folds = 5\n",
    "\n",
    "# Split the data into n_folds parts\n",
    "kf = KFold(n_splits=n_folds)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train the model on the training data and evaluate it on the test data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a18de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the categorical data into numeric form\n",
    "encoder = LabelEncoder()\n",
    "X_encoded = X.apply(encoder.fit_transform)\n",
    "\n",
    "# Initialize a list to store the predictions\n",
    "y_pred = []\n",
    "\n",
    "# Iterate over each row of the encoded data\n",
    "for i in range(X_encoded.shape[0]):\n",
    "    X_temp = X_encoded.drop(i, axis=0)\n",
    "    y_temp = y.drop(i, axis=0)\n",
    "\n",
    "    # Train the model on all but one row of the data\n",
    "    model.fit(X_temp, y_temp)\n",
    "\n",
    "    # Predict the target variable for the left-out row\n",
    "    y_pred.append(model.predict(X_encoded.iloc[i].values.reshape(1, -1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3006b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe6224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c478075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545d940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce294b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd95b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa410913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeaa5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97c734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42acf44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66d6d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de2cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
